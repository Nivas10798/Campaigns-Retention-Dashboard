{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "927da5e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary functions (wget, BeautifulSoup, Pandas, requests)\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import wget\n",
    "import calendar\n",
    "import pickle\n",
    "import re\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import json\n",
    "import pandas as pd\n",
    "from sqlite3 import dbapi2 as sq3\n",
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import copy\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "from collections import OrderedDict\n",
    "PATHSTART = '.'\n",
    "pd.options.display.max_columns = None\n",
    "#import logging\n",
    "\n",
    "#logging.basicConfig(filename=\"myfile.txt\",level=logging.DEBUG)\n",
    "\n",
    "base_url = 'https://dumps.wikimedia.org/other/mediawiki_history/'\n",
    "elog_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8fbd93e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to generate list of directories\n",
    "def generate_dir_list(url, db): #change name\n",
    "    temp_url = url + db + '/'\n",
    "    index = requests.get(str(temp_url)).text\n",
    "    soup_index = BeautifulSoup(index, 'html.parser')\n",
    "    temp_list = [a['href'].strip('/') for a in soup_index.find_all('a') if a.has_attr('href') and a['href'] != '../']\n",
    "    return temp_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4cff9c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating base url\n",
    "wikidb_list = generate_dir_list(base_url, '')\n",
    "base_url = base_url + wikidb_list[1] +'/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9095f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# necessary lists for processing\n",
    "wikidbs_to_remove = ['donatewiki', 'wikimania2015wiki', 'wikimania2016wiki', 'wikimania2017wiki', 'testwiki', \n",
    "                     'test2wiki', 'testwikidatawiki', 'metawiki', 'mediawikiwiki', 'outreachwiki', 'incubatorwiki', \n",
    "                     'votewiki', 'loginwiki', 'sewikimedia', 'trwikimedia', 'uawikimedia', 'wbwikimedia' \n",
    "                     'amwikimedia', 'arwikimedia', 'bdwikimedia', 'bewikimedia', 'brwikimedia', 'cawikimedia', \n",
    "                     'cnwikimedia', 'cowikimedia', 'dkwikimedia', 'etwikimedia', 'fiwikimedia', 'hiwikimedia', \n",
    "                     'idwikimedia', 'maiwikimedia', 'mkwikimedia', 'nxwikimedia', 'nlwikimedia', 'nowikimedia', \n",
    "                     'nycwikimedia', 'nzwikimedia', 'plwikimedia', 'ptwikimedia', 'rswikimedia', 'ruwikimedia']\n",
    "\n",
    "monthwise_wikidb_files = ['wikidatawiki', 'commonswiki', 'enwiki']\n",
    "\n",
    "yearwise_wikidb_files = ['dewiki', 'frwiki', 'eswiki', 'itwiki', 'ruwiki', 'jawiki', 'viwiki', 'zhwiki', 'ptwiki', \n",
    "                  'enwiktionary', 'plwiki', 'nlwiki', 'svwiki', 'arwiki', 'shwiki', 'cebwiki', 'mgwiktionary',\n",
    "                  'fawiki', 'frwiktionary', 'ukwiki', 'hewiki', 'kowiki', 'srwiki', 'trwiki', 'huwiki', \n",
    "                  'cawiki', 'nowiki', 'fiwiki', 'cswiki', 'idwiki', 'rowiki', 'enwikisource', 'frwikisource',\n",
    "                  'ruwiktionary', 'dawiki', 'bgwiki', 'enwikinews', 'specieswiki', 'thwiki']\n",
    "\n",
    "non_single_file_wikidb = wikidbs_to_remove + monthwise_wikidb_files + yearwise_wikidb_files\n",
    "\n",
    "req_cols = ['wiki_db', 'event_entity', 'event_timestamp','event_user_text', 'event_user_registration_timestamp', \n",
    "            'revision_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "88ca997f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scraping url for column names \n",
    "wiki_page=requests.get('https://wikitech.wikimedia.org/wiki/Analytics/Data_Lake/Edits/Mediawiki_history_dumps')\n",
    "index = wiki_page.text\n",
    "soup = BeautifulSoup(index, 'html.parser')\n",
    "sch_tbl = soup.select(\"h2:has(#Schema_details) + table\")\n",
    "df = pd.read_html(str(sch_tbl))[0]\n",
    "\n",
    "all_cols = df['Field name'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ce3977e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "838"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# modification to the list\n",
    "wikidb_list = generate_dir_list(base_url, '')\n",
    "\n",
    "# list of wikis having single files\n",
    "single_file_wikidb = [x for x in wikidb_list if x not in non_single_file_wikidb]\n",
    "len(single_file_wikidb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3ece2051",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "connected to newusers table of master_db\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_name</th>\n",
       "      <th>cohort</th>\n",
       "      <th>campaign</th>\n",
       "      <th>cat</th>\n",
       "      <th>country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PjeterGj</td>\n",
       "      <td>2016</td>\n",
       "      <td>WLE</td>\n",
       "      <td>Images_from_Wiki_Loves_Earth_2016_in_Albania</td>\n",
       "      <td>Albania</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Joana Balani</td>\n",
       "      <td>2016</td>\n",
       "      <td>WLE</td>\n",
       "      <td>Images_from_Wiki_Loves_Earth_2016_in_Albania</td>\n",
       "      <td>Albania</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Juela Metushi</td>\n",
       "      <td>2016</td>\n",
       "      <td>WLE</td>\n",
       "      <td>Images_from_Wiki_Loves_Earth_2016_in_Albania</td>\n",
       "      <td>Albania</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>JuljanGjura</td>\n",
       "      <td>2016</td>\n",
       "      <td>WLE</td>\n",
       "      <td>Images_from_Wiki_Loves_Earth_2016_in_Albania</td>\n",
       "      <td>Albania</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Jurgen Troka</td>\n",
       "      <td>2016</td>\n",
       "      <td>WLE</td>\n",
       "      <td>Images_from_Wiki_Loves_Earth_2016_in_Albania</td>\n",
       "      <td>Albania</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146225</th>\n",
       "      <td>Scorpione 68</td>\n",
       "      <td>2014</td>\n",
       "      <td>WLM</td>\n",
       "      <td>Images_from_Wiki_Loves_Monuments_2014</td>\n",
       "      <td>with_no_country</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146226</th>\n",
       "      <td>Scorpione1977</td>\n",
       "      <td>2014</td>\n",
       "      <td>WLM</td>\n",
       "      <td>Images_from_Wiki_Loves_Monuments_2014</td>\n",
       "      <td>with_no_country</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146227</th>\n",
       "      <td>Scrant26</td>\n",
       "      <td>2014</td>\n",
       "      <td>WLM</td>\n",
       "      <td>Images_from_Wiki_Loves_Monuments_2014</td>\n",
       "      <td>with_no_country</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146228</th>\n",
       "      <td>Tanline</td>\n",
       "      <td>2014</td>\n",
       "      <td>WLM</td>\n",
       "      <td>Images_from_Wiki_Loves_Monuments_2014</td>\n",
       "      <td>with_no_country</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146229</th>\n",
       "      <td>NiaPol</td>\n",
       "      <td>2018</td>\n",
       "      <td>WLE</td>\n",
       "      <td>Images_from_Wiki_Loves_Earth_2018</td>\n",
       "      <td>with_no_country</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>146230 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            user_name  cohort campaign  \\\n",
       "0            PjeterGj    2016      WLE   \n",
       "1        Joana Balani    2016      WLE   \n",
       "2       Juela Metushi    2016      WLE   \n",
       "3         JuljanGjura    2016      WLE   \n",
       "4        Jurgen Troka    2016      WLE   \n",
       "...               ...     ...      ...   \n",
       "146225   Scorpione 68    2014      WLM   \n",
       "146226  Scorpione1977    2014      WLM   \n",
       "146227       Scrant26    2014      WLM   \n",
       "146228        Tanline    2014      WLM   \n",
       "146229         NiaPol    2018      WLE   \n",
       "\n",
       "                                                 cat          country  \n",
       "0       Images_from_Wiki_Loves_Earth_2016_in_Albania          Albania  \n",
       "1       Images_from_Wiki_Loves_Earth_2016_in_Albania          Albania  \n",
       "2       Images_from_Wiki_Loves_Earth_2016_in_Albania          Albania  \n",
       "3       Images_from_Wiki_Loves_Earth_2016_in_Albania          Albania  \n",
       "4       Images_from_Wiki_Loves_Earth_2016_in_Albania          Albania  \n",
       "...                                              ...              ...  \n",
       "146225         Images_from_Wiki_Loves_Monuments_2014  with_no_country  \n",
       "146226         Images_from_Wiki_Loves_Monuments_2014  with_no_country  \n",
       "146227         Images_from_Wiki_Loves_Monuments_2014  with_no_country  \n",
       "146228         Images_from_Wiki_Loves_Monuments_2014  with_no_country  \n",
       "146229             Images_from_Wiki_Loves_Earth_2018  with_no_country  \n",
       "\n",
       "[146230 rows x 5 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reading the new users csv file\n",
    "master_db = sq3.connect('master.db')\n",
    "new_users_df = pd.read_sql_query(\"\"\"SELECT * FROM newusers\"\"\", master_db)\n",
    "print('connected to newusers table of master_db')\n",
    "new_users_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eec36fc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "146230"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# list of new users through all the years of all the campaigns\n",
    "new_users_list = list(new_users_df['user_name'])\n",
    "len(new_users_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d16eb309",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to delete a file\n",
    "def del_file(file):\n",
    "    if os.path.exists(file) == True:\n",
    "        os.remove(file)\n",
    "        print('\\n' + file + ' is deleted')\n",
    "    else:\n",
    "        print('No such file found')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b93e86b3",
   "metadata": {},
   "source": [
    "# Downloading and processing single file wiki databases\n",
    "\n",
    "In this, we will make first make a list of file urls and download them batchwise. The size of the batches will be given before running the code along with number of iterations in a single execution. \n",
    "From every downloaded file, we take the data or list of users from users table and append them to alledits table  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e22ed34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to download and process single wiki database files\n",
    "def append_single_file_wikidb(wikis_list, status_file, errors_file, no_of_files, \n",
    "                       no_of_batches, users_list, table_name, db_object):\n",
    "    \n",
    "    exp_dict = {}\n",
    "    status_key = 0\n",
    "    \n",
    "    # generating file urls for all the wikis in the given list\n",
    "    if os.path.exists('singlewiki_fileurls.csv') == True:\n",
    "        single_wiki_url_list = pd.read_csv('singlewiki_fileurls.csv')['single_wiki_url'].to_list()\n",
    "    else:\n",
    "        single_wiki_url_list = []\n",
    "        for db in wikis_list:\n",
    "            try:\n",
    "                #print(db)\n",
    "                file_name = generate_dir_list(base_url, db)\n",
    "                #print(file_name)\n",
    "                file_url = base_url + str(db) + '/' + file_name[0]\n",
    "                #print(file_url)\n",
    "                single_wiki_url_list.append(file_url)\n",
    "            except Exception as err:\n",
    "                exp_dict[db] = str(err)\n",
    "                pass\n",
    "            dict = {'single_wiki_url':single_wiki_url_list}\n",
    "            temp_df = pd.DataFrame(dict) \n",
    "            temp_df.to_csv('singlewiki_fileurls.csv')\n",
    "            del temp_df        \n",
    "\n",
    "    list_len = len(single_wiki_url_list)\n",
    "    #print(list_len)\n",
    "    \n",
    "    #checking for last processed batch\n",
    "    try:\n",
    "        with open(status_file, 'rb') as handle:\n",
    "            status_key = pickle.load(handle)\n",
    "        j = status_key\n",
    "    except Exception as err:\n",
    "        exp_dict[status_file] = str(err)\n",
    "        pass    \n",
    "\n",
    "    # checking if all wikis the are processed or not\n",
    "    try:\n",
    "        if list_len == j:\n",
    "            print('Completed')\n",
    "            del_file('singlewiki_fileurls.csv')\n",
    "            raise StopExecution\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # creating batches of urls\n",
    "    temp_list = [single_wiki_url_list[index:index + no_of_files] for index in \n",
    "                 range(0, len(single_wiki_url_list), no_of_files)]\n",
    "    \n",
    "    # making set of batches to download and process the files\n",
    "    for j in range(status_key, status_key + no_of_batches):\n",
    "        try:\n",
    "            for item in temp_list[j]:\n",
    "                print(item)\n",
    "                try:\n",
    "                    file_name = wget.download(item)\n",
    "                except Exception as err:\n",
    "                    exp_dict[item] = str(err)\n",
    "                    pass\n",
    "                try:\n",
    "                    data = pd.read_csv(file_name, names = all_cols, compression='bz2', sep='\\t', \n",
    "                                       low_memory=False, on_bad_lines = 'warn', chunksize=1000000)\n",
    "                    df = pd.DataFrame(columns= req_cols)\n",
    "                    for temp_df in data:\n",
    "                        df = pd.concat([df, temp_df[req_cols]], ignore_index=True)\n",
    "                        print(file_name + 'Stage:', df.index.max())\n",
    "                    users_filter = df['event_user_text'].isin(users_list)\n",
    "                    df = df[users_filter]\n",
    "                    df.to_sql(table_name, db_object, if_exists='append', index=False)\n",
    "                    del df\n",
    "                except Exception as err:\n",
    "                    exp_dict[file_name] = str(err)\n",
    "                    pass\n",
    "                try:\n",
    "                    if os.path.exists(file_name) == True:\n",
    "                        os.remove(file_name)\n",
    "                except Exception as err:\n",
    "                    file_loc = os.getcwd()\n",
    "                    exp_dict[file_loc] = str(err)\n",
    "                    pass\n",
    "        except Exception as err:\n",
    "            exp_dict[status_key] = str(err)\n",
    "            pass\n",
    "        status_key = j + 1\n",
    "        with open(status_file, 'wb') as stat_file:\n",
    "            pickle.dump(status_key, stat_file)\n",
    "        stat_file.close()\n",
    "    print('Batch completed')\n",
    "    with open(errors_file, 'a+') as err_file:\n",
    "         err_file.write(json.dumps(exp_dict))\n",
    "    err_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "514cc4b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# running the function\n",
    "append_single_file_wikidb(single_file_wikidb, no_of_files=15, status_file = 'single_status.pickle', \n",
    "                   errors_file = 'single_error_log.txt', no_of_batches=10, \n",
    "                   users_list=new_users_list, table_name=\"alledits\", db_object=master_db)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c3f6cc1",
   "metadata": {},
   "source": [
    "# Downloading and processing wiki databases which are segregated yearwise\n",
    "\n",
    "In this, we will make first make a list of file urls and download them batchwise. The size of the batches will be given before running the code along with number of iterations in a single execution. \n",
    "From every downloaded file, we take the data or list of users from users table and append them to alledits table  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c598a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get year of the wiki database file\n",
    "def year_file(db_name, file_name):\n",
    "    i = file_name.split(db_name)[1].strip('.-')\n",
    "    year = re.compile(r'\\d{4}').findall(i)[0]\n",
    "    return int(year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a03e0050",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to download and process yearwise segregated wiki database files\n",
    "def append_year_wikis(wikis_list, year, status_file, errors_file, no_of_files, \n",
    "                       no_of_batches, users_list, table_name, db_object):\n",
    "    \n",
    "    exp_dict = {}\n",
    "    status_key = 0\n",
    "    \n",
    "    # generating file urls for all the wikis in the given list\n",
    "    if os.path.exists('yearwisewiki_fileurls.csv') == True:\n",
    "        yearwise_wiki_url_list = pd.read_csv('yearwisewiki_fileurls.csv')['year_wiki_url'].to_list()\n",
    "    else:\n",
    "        yearwise_wiki_url_list = []\n",
    "        for db in wikis_list:\n",
    "            try:\n",
    "                wiki_all_list = generate_dir_list(base_url, db)\n",
    "            except Exception as err:\n",
    "                exp_dict[db] = str(err)\n",
    "                pass\n",
    "\n",
    "            # checking if the file is within specified year \n",
    "            for i in wiki_all_list:\n",
    "                file_year = year_file(db, i)\n",
    "                try:\n",
    "                    if file_year >= year and file_year < 2023:\n",
    "                        file_url = base_url + db + '/' + i\n",
    "                        yearwise_wiki_url_list.append(file_url)\n",
    "                except Exception as err:\n",
    "                    exp_dict[i] = str(err)\n",
    "                    pass\n",
    "                dict = {'year_wiki_url':yearwise_wiki_url_list}\n",
    "                temp_df = pd.DataFrame(dict) \n",
    "                temp_df.to_csv('yearwisewiki_fileurls.csv')\n",
    "                del temp_df\n",
    "\n",
    "    list_len = len(yearwise_wiki_url_list)\n",
    "    #print(list_len)\n",
    "    \n",
    "    #checking for last processed batch\n",
    "    try:\n",
    "        with open(status_file, 'rb') as handle:\n",
    "            status_key = pickle.load(handle)\n",
    "        j = status_key\n",
    "    except Exception as err:\n",
    "        exp_dict[status_file] = str(err)\n",
    "        pass    \n",
    "\n",
    "    # checking if all wikis the are processed or not\n",
    "    try:\n",
    "        if list_len == j:\n",
    "            print('Completed')\n",
    "            raise StopExecution\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # creating batches of urls\n",
    "    temp_list = [yearwise_wiki_url_list[index:index + no_of_files] for index in \n",
    "                 range(0, len(yearwise_wiki_url_list), no_of_files)]\n",
    "    \n",
    "    # making set of batches to download and process the files\n",
    "    for j in range(status_key, status_key + no_of_batches):\n",
    "        try:\n",
    "            for item in temp_list[j]:\n",
    "                print(item)\n",
    "                try:\n",
    "                    file_name = wget.download(item), bar=None)\n",
    "                except Exception as err:\n",
    "                    exp_dict[item] = str(err)\n",
    "                    pass\n",
    "                try:\n",
    "                    data = pd.read_csv(file_name, names = all_cols, compression='bz2', sep='\\t', \n",
    "                                       low_memory=False, on_bad_lines = 'warn', chunksize=1000000)\n",
    "                    df = pd.DataFrame(columns= req_cols)\n",
    "                    for temp_df in data:\n",
    "                        df = pd.concat([df, temp_df[req_cols]], ignore_index=True)\n",
    "                        print(file_name + 'Stage:', df.index.max())\n",
    "                    users_filter = df['event_user_text'].isin(users_list)\n",
    "                    df = df[users_filter]\n",
    "                    df.to_sql(table_name, db_object, if_exists='append', index=False)\n",
    "                    del df\n",
    "                except Exception as err:\n",
    "                    exp_dict[file_name] = str(err)\n",
    "                    pass\n",
    "                try:\n",
    "                    if os.path.exists(file_name) == True:\n",
    "                        os.remove(file_name)\n",
    "                except Exception as err:\n",
    "                    file_loc = os.getcwd()\n",
    "                    exp_dict[file_loc] = str(err)\n",
    "                    pass\n",
    "        except Exception as err:\n",
    "            exp_dict[status_key] = str(err)\n",
    "            pass\n",
    "    status_key = j + 1\n",
    "    with open(status_file, 'wb') as stat_file:\n",
    "        pickle.dump(status_key, stat_file)\n",
    "    stat_file.close()\n",
    "    with open(errors_file, 'a+') as err_file:\n",
    "         err_file.write(json.dumps(exp_dict))\n",
    "    err_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96debbe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# running the function\n",
    "append_year_wikis(yearwise_wikidb_files, year=2010, no_of_files=8, status_file = 'year_status.pickle', \n",
    "                   errors_file = 'error_log.txt', no_of_batches=5, users_list=new_users_list, \n",
    "                   table_name=\"alledits\", db_object=master_db)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "646b3e7a",
   "metadata": {},
   "source": [
    "# Downloading and processing wiki databases which are segregated monthwise\n",
    "\n",
    "In this, we will make first make a list of file urls and download them batchwise. The size of the batches will be given before running the code along with number of iterations in a single execution. \n",
    "We will first check \n",
    "From every downloaded file, we take the data or list of users from users table and append them to alledits table  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "724834f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get month and year of wiki database file\n",
    "def year_month_file(db_name, file_name):\n",
    "    i = file_name.split(db_name)[1].strip('.-')\n",
    "    year = re.compile(r'\\d{4}').findall(i)[0]\n",
    "    j = file_name.split(year)[1].strip('.-')\n",
    "    month = re.compile(r'\\d{2}').findall(j)[0]\n",
    "    # month_name = calendar.month_name[int(month)]\n",
    "    return int(year), int(month)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e97291b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # function to download and process monthwise segregated wiki database files\n",
    "def append_month_wikis(wikis_list, year, month, status_file, errors_file, no_of_files, \n",
    "                       no_of_batches, users_list, table_name, db_object):\n",
    "    \n",
    "    exp_dict = {}\n",
    "    status_key = 0\n",
    "    \n",
    "    # generating file urls for all the wikis in the given list\n",
    "    if os.path.exists('monthwisewiki_fileurls.csv') == True:\n",
    "        monthwise_wiki_url_list = pd.read_csv('monthwisewiki_fileurls.csv')['month_wiki_url'].to_list()\n",
    "    else:\n",
    "        monthwise_wiki_url_list = []\n",
    "        for db in wikis_list:\n",
    "            try:\n",
    "                wiki_all_list = generate_dir_list(base_url, db)\n",
    "            except Exception as err:\n",
    "                exp_dict[db] = str(err)\n",
    "                pass\n",
    "\n",
    "            # checking if the file is within specified year and month \n",
    "            for i in wiki_all_list:\n",
    "                file_year, file_month = year_month_file(db, i)\n",
    "                try:\n",
    "                    if (file_year == year and file_month >= month):\n",
    "                        file_url = base_url + db + '/' + i\n",
    "                        monthwise_wiki_url_list.append(file_url)\n",
    "                    elif (file_year >= year+1):\n",
    "                        file_url = base_url + db + '/' + i\n",
    "                        monthwise_wiki_url_list.append(file_url)\n",
    "                except Exception as err:\n",
    "                    exp_dict[i] = str(err)\n",
    "                    pass\n",
    "                dict = {'month_wiki_url':monthwise_wiki_url_list}\n",
    "                temp_df = pd.DataFrame(dict) \n",
    "                temp_df.to_csv('monthwisewiki_fileurls.csv')\n",
    "                del temp_df\n",
    "                \n",
    "    list_len = len(monthwise_wiki_url_list)\n",
    "    #print(list_len)\n",
    "    \n",
    "    #checking for last processed batch\n",
    "    try:\n",
    "        with open(status_file, 'rb') as handle:\n",
    "            status_key = pickle.load(handle)\n",
    "        j = status_key\n",
    "    except Exception as err:\n",
    "        exp_dict[status_file] = str(err)\n",
    "        pass    \n",
    "\n",
    "    # checking if all wikis the are processed or not\n",
    "    try:\n",
    "        if list_len == j:\n",
    "            print('Completed')\n",
    "            raise StopExecution\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # creating batches of urls\n",
    "    temp_list = [monthwise_wiki_url_list[index:index + no_of_files] for index in \n",
    "                 range(0, len(monthwise_wiki_url_list), no_of_files)]\n",
    "    \n",
    "    # making set of batches to download and process the files\n",
    "    for j in range(status_key, status_key + no_of_batches):\n",
    "        try:\n",
    "            for item in temp_list[j]:\n",
    "                print(item)\n",
    "                try:\n",
    "                    file_name = wget.download(item)\n",
    "                except Exception as err:\n",
    "                    exp_dict[item] = str(err)\n",
    "                    pass\n",
    "                try:\n",
    "                    data = pd.read_csv(file_name, names = all_cols, compression='bz2', sep='\\t', \n",
    "                                       low_memory=False, on_bad_lines = 'warn', chunksize=1000000)\n",
    "                    df = pd.DataFrame(columns= req_cols)\n",
    "                    for temp_df in data:\n",
    "                        df = pd.concat([df, temp_df[req_cols]], ignore_index=True)\n",
    "                        print(file_name + 'Stage:', df.index.max())\n",
    "                    users_filter = df['event_user_text'].isin(users_list)\n",
    "                    df = df[users_filter]\n",
    "                    df.to_sql(table_name, db_object, if_exists='append', index=False)\n",
    "                    del df\n",
    "                except Exception as err:\n",
    "                    exp_dict[file_name] = str(err)\n",
    "                    pass\n",
    "                try:\n",
    "                    if os.path.exists(file_name) == True:\n",
    "                        os.remove(file_name)\n",
    "                except Exception as err:\n",
    "                    file_loc = os.getcwd()\n",
    "                    exp_dict[file_loc] = str(err)\n",
    "                    pass\n",
    "        except Exception as err:\n",
    "            exp_dict[status_key] = str(err)\n",
    "            pass\n",
    "    status_key = j + 1\n",
    "    with open(status_file, 'wb') as stat_file:\n",
    "        pickle.dump(status_key, stat_file)\n",
    "    stat_file.close()\n",
    "    with open(errors_file, 'a+') as err_file:\n",
    "         err_file.write(json.dumps(exp_dict))\n",
    "    err_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f0f22a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# running the function\n",
    "append_month_wikis(monthwise_wikidb_files, year=2017, month=9, no_of_files=12, status_file = 'month_status.pickle', \n",
    "                   errors_file = 'month_error_log.txt', no_of_batches=1, \n",
    "                   users_list=new_users_list, table_name=\"alledits\", db_object=master_db)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
